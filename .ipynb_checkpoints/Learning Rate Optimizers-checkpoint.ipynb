{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we went over learning rates and parameter tuning with gradient descent. In this document, we will go over optimizers for learning rate so that we can avoid parameter tuning, which is a very lengthy trial and error process. An optimizer is a method of updating the learning rate iteratively as a function of another variable.\n",
    "\n",
    "We will go over and demo the following optimizers:\n",
    "1. AdaGrad\n",
    "2. RMSprop\n",
    "3. Adam\n",
    "4. AdaDelta\n",
    "5. Nadam\n",
    "6. AdaMax\n",
    "7. AMSgrad\n",
    "\n",
    "There are many other methods for computing the learning rate, and we saw a few of them in the variants we explored. However, now we want to turn our attention to some more popular methods that are often paired with SGD in autoML/ML systems, like Keras, Google Cloud AutoML, sklearn. We will explore the listed techniques with SGD, which are optimizers in found in Keras. We will implement the first two using Stochastic Gradient Descent, and the rest of the optimizers are designed to be used with Nesterov's Accelerated Gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad and RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic learning rate variant is AdaGrad. AdaGrad updates the learning rate by dividing by a factor of a new variable $S_{t+1} = S_{t} + |\\nabla F(w^{(t)})|^{2}$. Thus we update by $$w^{(t+1)} = w^{(t)} - \\frac{\\mu}{\\sqrt{S_{t} + \\epsilon}} \\nabla F(w^{(t)})$$ $$S_{t+1} = S_{t} + |\\nabla F(w^{(t)})|^{2}$$\n",
    "\n",
    "\n",
    "RMSprop uses this same idea, but it takes the average of all the previous iterates of $S$. Thus our update is: $$w^{(t+1)} = w^{(t)} - \\frac{\\mu}{\\sqrt{S_{t} + \\epsilon}} \\nabla F(w^{(t)})$$ $$S_{t+1} = \\beta S_{t} + | (1- \\beta) \\nabla F(w^{(t)})|^{2}$$ Here, we choose $\\beta \\in [0,1)$. A typical choice for $\\beta$ is $\\beta =0.9$. We will apply these algorithms with SGD and NAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
